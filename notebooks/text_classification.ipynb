{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification with torchtext\n",
    "* https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "* ngram based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import text_classification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ag_news_csv.tar.gz: 11.8MB [00:00, 65.1MB/s]\n",
      "120000lines [00:06, 18489.60lines/s]\n",
      "120000lines [00:14, 8537.20lines/s]\n",
      "7600lines [00:00, 9216.29lines/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "NGRAMS = 2\n",
    "\n",
    "if not os.path.isdir('/ndata/NLP/text_classification'):\n",
    "    os.mkdir('/ndata/NLP/text_classification')\n",
    "    \n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='/ndata/NLP/text_classification', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range = 0.5\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch functions\n",
    "* since text entries have different lenghts, we hadd the text lenght ('offset') and pass that to the embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # final offsets via cumsum, note you don't need an offset for the last entry     \n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n",
    "\n",
    "\n",
    "def train_test(dataset_subset, is_train):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "    data_loader = DataLoader(dataset_subset, batch_size=BATCH_SIZE, shuffle=is_train,\n",
    "                     collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, labels) in enumerate(data_loader):\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "        \n",
    "        if is_train:\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = model(text, offsets)\n",
    "                loss = criterion(output, labels)\n",
    "        \n",
    "        batch_loss += loss.item()\n",
    "        batch_acc += (output.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    if is_train:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return batch_loss / (len(dataset_subset)), batch_acc / (len(dataset_subset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main epochs loop\n",
    "* split dataset\n",
    "* run epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 16 seconds\n",
      "\tLoss: 0.0124(train)\t|\tAcc: 93.4%(train)\n",
      "\tLoss: 0.0148(valid)\t|\tAcc: 91.6%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 15 seconds\n",
      "\tLoss: 0.0070(train)\t|\tAcc: 96.3%(train)\n",
      "\tLoss: 0.0128(valid)\t|\tAcc: 93.6%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 15 seconds\n",
      "\tLoss: 0.0039(train)\t|\tAcc: 98.1%(train)\n",
      "\tLoss: 0.0128(valid)\t|\tAcc: 93.9%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 15 seconds\n",
      "\tLoss: 0.0023(train)\t|\tAcc: 99.0%(train)\n",
      "\tLoss: 0.0142(valid)\t|\tAcc: 93.7%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 15 seconds\n",
      "\tLoss: 0.0014(train)\t|\tAcc: 99.4%(train)\n",
      "\tLoss: 0.0141(valid)\t|\tAcc: 93.9%(valid)\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_test(sub_train_, True)\n",
    "    valid_loss, valid_acc = train_test(sub_valid_, False)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on random snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        \n",
    "        text = text.to(device)\n",
    "        output = model(text, torch.tensor([0]).to(device))\n",
    "        return output.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Sports news\n",
      "This is Sci/Tec news\n"
     ]
    }
   ],
   "source": [
    "vocab = train_dataset.get_vocab()\n",
    "\n",
    "ex1 = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "ex2 = \"António Guterrez, the United Nations secretary general, \\\n",
    "contrasted the “leadership” and “mobilisation” shown by the world’s \\\n",
    "youth on the climate emergency with the lack of action by governments, \\\n",
    "which were failing to keep up with the urgency of the problem \\\n",
    "despite increasing signs that the climate was reaching breakdown.\"\n",
    "\n",
    "\n",
    "# model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is %s news\" %ag_news_label[predict(ex1, model, vocab, 2)])\n",
    "print(\"This is %s news\" %ag_news_label[predict(ex2, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
