{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with an attention transformer\n",
    "* sentiment bit adapted from: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "* transformer from: bloem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import text_classification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ag_news_csv.tar.gz: 11.8MB [00:00, 15.9MB/s]\n",
      "120000lines [00:09, 13085.34lines/s]\n",
      "120000lines [00:17, 7040.62lines/s]\n",
      "7600lines [00:01, 7261.86lines/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "NGRAMS = 2\n",
    "\n",
    "if not os.path.isdir('../data/NLP/text_classification'):\n",
    "    os.mkdir('../data/NLP/text_classification')\n",
    "    \n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='../data/NLP/text_classification', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range = 0.5\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, k, num_heads=8):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # determine queries, keys, values\n",
    "        self.key_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.query_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.value_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        \n",
    "        # project down all cat-ed heads\n",
    "        self.unify_layer = nn.Linear(heads * k, k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get batch size, t sentences of k items\n",
    "        b_sz, t_sz, k_sz = x.size()\n",
    "        h_sz = self.num_heads\n",
    "        \n",
    "        keys = self.key_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        queries = self.query_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        values = self.value_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "    \n",
    "        # compute dot products (k x k). Same op for every head, so fold in to the\n",
    "        # batch dim\n",
    "        # q, k, v, (b, t, h, k) -> (b, h, t, k) -> (bh, t, k)\n",
    "        # and for the key (bh, t, k) -> (bh, k, t) to be able to use bmm\n",
    "        #\n",
    "        keys = keys.transpose(1, 2).continuous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        queries = queries.transpose(1, 2).continuous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        values = values.transpose(1, 2).continuous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # intermediate scaling\n",
    "        queries = queries / ( k ** (1./4.))\n",
    "        keys = keys / ( k ** (1./4.))\n",
    "        \n",
    "        # final transpose for the bmm, out -> (b*h, t, t)\n",
    "        raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        \n",
    "        # row wise softmax normalize\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        \n",
    "        # apply self attention to the values\n",
    "        out = torch.bmm(weights, values).view(b_sz, h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # Unify attention heads\n",
    "        # reshuffle (b, h, t, k) -> (b, t, h, k) -> (b, t, h*k) with all the heads catted\n",
    "        # ontop of each other to be able to down project\n",
    "        out = out.transpose(1, 2).continuous().view(b_sz, t_sz, h_sz * k_sz)\n",
    "        \n",
    "        # project down\n",
    "        out = self.unify_layer(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = AttentionLayer(k, num_heads)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(k)\n",
    "        self.layer_norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(k, 4 * k),\n",
    "            nn.Relu(),\n",
    "            nn.Linear(4 * k, k)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        x_att = self.attention(x)\n",
    "        # Residual + norm\n",
    "        x = self.layer_norm1(x + x_att)\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "        out = self.layer_norm2(x + x_mlp)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, k, num_heads, depth, seq_length, num_tokens, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        # Embedding tokens and position layers\n",
    "        self.token_embed_layer = nn.Embedding(num_tokens, k)\n",
    "        self.position_embed_layer = nn.Embedding(seq_length, k)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.tf_network = []\n",
    "        for _ in range(depth):\n",
    "            tf_network.append(TransformerBlock(k, num_heads))\n",
    "\n",
    "        self.tf_network = nn.Sequential(*self.network)\n",
    "        \n",
    "        # Sequence to class output\n",
    "        self.output_layer = nn.Linear(k, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in (b, t) tensor with int values representing words\n",
    "        # out (b, c) tensor logprobs over c classes\n",
    "        \n",
    "        # generate token embeddings\n",
    "        tokens = self.token_embed_layer(x)\n",
    "        \n",
    "        b_sz, t_sz, k_sz = tokens.size()\n",
    "        \n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(t_sz)\n",
    "        positions = self.position_embed_layer(positions)[None, :, :].expand(b_sz, t_sz, k_sz)\n",
    "        \n",
    "        x = tokens + positions\n",
    "        \n",
    "        # Transformer forward\n",
    "        x = self.tf_network(x)\n",
    "        \n",
    "        # Average pool over t dimension and project to class probabilities\n",
    "        x = self.output_layer(x.mean(dim=1))\n",
    "        \n",
    "        # Optional (auto-regressive) transformer\n",
    "        # no looking ahead, enforce via mask, prior to softmax\n",
    "#         indices = torch.triu_indices(t, t, offset=1)\n",
    "#         x[:, indices[0], indices[1]] = float('-inf')\n",
    "        \n",
    "        \n",
    "        out = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "# k, num_heads, depth, seq_length, num_tokens, num_\n",
    "seq_length = \n",
    "model = Transformer(EMBED_DIM, 1, 1,   seq_length, VOCAB_SIZE, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # final offsets via cumsum, note you don't need an offset for the last entry     \n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n",
    "\n",
    "\n",
    "def train_test(dataset_subset, is_train):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "    data_loader = DataLoader(dataset_subset, batch_size=BATCH_SIZE, shuffle=is_train,\n",
    "                     collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, labels) in enumerate(data_loader):\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        text, offsets, labels = text.to(device), offsets.to(device), labels.to(device)\n",
    "        \n",
    "        if is_train:\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = model(text, offsets)\n",
    "                loss = criterion(output, labels)\n",
    "        \n",
    "        batch_loss += loss.item()\n",
    "        batch_acc += (output.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    if is_train:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return batch_loss / (len(dataset_subset)), batch_acc / (len(dataset_subset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main epoch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_test(sub_train_, True)\n",
    "    valid_loss, valid_acc = train_test(sub_valid_, False)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        \n",
    "        text = text.to(device)\n",
    "        output = model(text, torch.tensor([0]).to(device))\n",
    "        return output.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_dataset.get_vocab()\n",
    "\n",
    "ex1 = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "ex2 = \"António Guterrez, the United Nations secretary general, \\\n",
    "contrasted the “leadership” and “mobilisation” shown by the world’s \\\n",
    "youth on the climate emergency with the lack of action by governments, \\\n",
    "which were failing to keep up with the urgency of the problem \\\n",
    "despite increasing signs that the climate was reaching breakdown.\"\n",
    "\n",
    "\n",
    "# model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is %s news\" %ag_news_label[predict(ex1, model, vocab, 2)])\n",
    "print(\"This is %s news\" %ag_news_label[predict(ex2, model, vocab, 2)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
