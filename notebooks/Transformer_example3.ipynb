{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.peterbloem.nl/blog/transformers\n",
    "# https://github.com/pbloem/former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math, copy, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention module\n",
    "* h attention heads, as h seperate sets of the three matrices W_q, W_k, W_v\n",
    "* for efficiency, we combine thise into three single k x kh matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, k, num_heads=8):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # determine queries, keys, values\n",
    "        self.key_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.query_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.value_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        \n",
    "        # project down all cat-ed heads\n",
    "        self.unify_layer = nn.Linear(heads * k, k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get batch size, t sentences of k items\n",
    "        b_sz, t_sz, k_sz = x.size()\n",
    "        h_sz = self.num_heads\n",
    "        \n",
    "        keys = self.key_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        queries = self.query_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        values = self.value_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "    \n",
    "        # compute dot products (k x k). Same op for every head, so fold in to the\n",
    "        # batch dim\n",
    "        # q, k, v, (b, t, h, k) -> (b, h, t, k) -> (bh, t, k)\n",
    "        # and for the key (bh, t, k) -> (bh, k, t) to be able to use bmm\n",
    "        #\n",
    "        keys = keys.transpose(1, 2).continuous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        queries = queries.transpose(1, 2).continuous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        values = values.transpose(1, 2).continuous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # intermediate scaling\n",
    "        queries = queries / ( k ** (1./4.))\n",
    "        keys = keys / ( k ** (1./4.))\n",
    "        \n",
    "        # final transpose for the bmm, out -> (b*h, t, t)\n",
    "        raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        \n",
    "        # row wise softmax normalize\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        \n",
    "        # apply self attention to the values\n",
    "        out = torch.bmm(weights, values).view(b_sz, h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # Unify attention heads\n",
    "        # reshuffle (b, h, t, k) -> (b, t, h, k) -> (b, t, h*k) with all the heads catted\n",
    "        # ontop of each other to be able to down project\n",
    "        out = out.transpose(1, 2).continuous().view(b_sz, t_sz, h_sz * k_sz)\n",
    "        \n",
    "        # project down\n",
    "        out = self.unify_layer(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = AttentionLayer(k, num_heads)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(k)\n",
    "        self.layer_norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(k, 4 * k),\n",
    "            nn.Relu(),\n",
    "            nn.Linear(4 * k, k)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        x_att = self.attention(x)\n",
    "        # Residual + norm\n",
    "        x = self.layer_norm1(x + x_att)\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "        out = self.layer_norm2(x + x_mlp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
